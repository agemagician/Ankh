{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb6aac13",
   "metadata": {},
   "source": [
    "### Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 7\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "import ankh\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from datasets import load_dataset\n",
    "\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from scipy import stats\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838401df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "916de6b1",
   "metadata": {},
   "source": [
    "### Select the available device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Available device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "079738e9",
   "metadata": {},
   "source": [
    "### Load Ankh large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = ankh.load_large_model()\n",
    "model.eval()\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters:\", get_num_params(model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e8b01ed",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ff8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"proteinea/secondary_structure_prediction\"\n",
    "training_dataset = load_dataset(name, data_files={'train': ['training_hhblits.csv']})\n",
    "casp12_dataset = load_dataset(name, data_files={'test': ['CASP12.csv']})\n",
    "casp14_dataset = load_dataset(name, data_files={'test': ['CASP14.csv']})\n",
    "ts115_dataset = load_dataset(name, data_files={'test': ['TS115.csv']})\n",
    "cb513_dataset = load_dataset(name, data_files={'test': ['CB513.csv']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_column_name = 'input'\n",
    "labels_column_name = 'dssp3' # You can change it to \"dssp8\" if you want to work with 8 states.\n",
    "disorder_column_name = 'disorder'\n",
    "training_sequences, training_labels, training_disorder = (\n",
    "    training_dataset['train'][input_column_name], \n",
    "    training_dataset['train'][labels_column_name],\n",
    "    training_dataset['train'][disorder_column_name]\n",
    ")\n",
    "\n",
    "\n",
    "casp12_sequences, casp12_labels, casp12_disorder = (\n",
    "    casp12_dataset['test'][input_column_name], \n",
    "    casp12_dataset['test'][labels_column_name],\n",
    "    casp12_dataset['test'][disorder_column_name]\n",
    ")\n",
    "\n",
    "casp14_sequences, casp14_labels, casp14_disorder = (\n",
    "    casp14_dataset['test'][input_column_name], \n",
    "    casp14_dataset['test'][labels_column_name],\n",
    "    casp14_dataset['test'][disorder_column_name]\n",
    ")\n",
    "\n",
    "ts115_sequences, ts115_labels, ts115_disorder = (\n",
    "    ts115_dataset['test'][input_column_name], \n",
    "    ts115_dataset['test'][labels_column_name],\n",
    "    ts115_dataset['test'][disorder_column_name]\n",
    ")\n",
    "\n",
    "cb513_sequences, cb513_labels, cb513_disorder = (\n",
    "    cb513_dataset['test'][input_column_name], \n",
    "    cb513_dataset['test'][labels_column_name],\n",
    "    cb513_dataset['test'][disorder_column_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(sequences, labels, disorder, max_length=None):\n",
    "    \n",
    "    sequences = [\"\".join(seq.split()) for seq in sequences]\n",
    "    \n",
    "    if max_length is None:\n",
    "        max_length = len(max(sequences, key=lambda x: len(x)))\n",
    "\n",
    "    seqs = [list(seq)[:max_length] for seq in sequences]\n",
    "    \n",
    "    labels = [\"\".join(label.split()) for label in labels]\n",
    "    labels = [list(label)[:max_length] for label in labels]\n",
    "    \n",
    "    disorder = [\" \".join(disorder.split()) for disorder in disorder]\n",
    "    disorder = [disorder.split()[:max_length] for disorder in disorder]\n",
    "    \n",
    "    assert len(seqs) == len(labels) == len(disorder)\n",
    "    return seqs, labels, disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cea65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_dataset(model, sequences, shift_left = 0, shift_right = -1):\n",
    "    inputs_embedding = []\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(sequences):\n",
    "            ids = tokenizer.batch_encode_plus([sample], add_special_tokens=True, \n",
    "                                              padding=True, is_split_into_words=True, \n",
    "                                              return_tensors=\"pt\")\n",
    "            embedding = model(input_ids=ids['input_ids'].to(device))[0]\n",
    "            embedding = embedding[0].detach().cpu().numpy()[shift_left:shift_right]\n",
    "            inputs_embedding.append(embedding)\n",
    "    return inputs_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb692b1b",
   "metadata": {},
   "source": [
    "### Preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences, training_labels, training_disorder = preprocess_dataset(training_sequences, \n",
    "                                                                            training_labels, \n",
    "                                                                            training_disorder)\n",
    "casp12_sequences, casp12_labels, casp12_disorder = preprocess_dataset(casp12_sequences, \n",
    "                                                                      casp12_labels, \n",
    "                                                                      casp12_disorder)\n",
    "\n",
    "casp14_sequences, casp14_labels, casp14_disorder = preprocess_dataset(casp14_sequences, \n",
    "                                                                      casp14_labels, \n",
    "                                                                      casp14_disorder)\n",
    "ts115_sequences, ts115_labels, ts115_disorder = preprocess_dataset(ts115_sequences, \n",
    "                                                                   ts115_labels, \n",
    "                                                                   ts115_disorder)\n",
    "cb513_sequences, cb513_labels, cb513_disorder = preprocess_dataset(cb513_sequences, \n",
    "                                                                   cb513_labels, \n",
    "                                                                   cb513_disorder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b3c15c7",
   "metadata": {},
   "source": [
    "### Extract sequences embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89efc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = embed_dataset(model, training_sequences[:10])\n",
    "casp12_embeddings = embed_dataset(model, casp12_sequences[:10])\n",
    "casp14_embeddings = embed_dataset(model, casp14_sequences[:10])\n",
    "ts115_embeddings = embed_dataset(model, ts115_sequences[:10])\n",
    "cb513_embeddings = embed_dataset(model, cb513_sequences[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f72ae5f",
   "metadata": {},
   "source": [
    "### Create unique tag for each state, in this current task we have only 3 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider each label as a tag for each token\n",
    "unique_tags = set(tag for doc in training_labels for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f0e434c",
   "metadata": {},
   "source": [
    "### Encode the tags in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c17635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(labels):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_encodings = encode_tags(training_labels)\n",
    "casp12_labels_encodings = encode_tags(casp12_labels)\n",
    "casp14_labels_encodings = encode_tags(casp14_labels)\n",
    "ts115_labels_encodings = encode_tags(ts115_labels)\n",
    "cb513_labels_encodings = encode_tags(cb513_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "814ab0e5",
   "metadata": {},
   "source": [
    "### Mask disordered tokens, Mask is set to -100 which is the default value for `ignore_index` in the cross entropy loss in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_disorder(labels, masks):\n",
    "    for label, mask in zip(labels,masks):\n",
    "        for i, disorder in enumerate(mask):\n",
    "            if disorder == \"0.0\":\n",
    "                label[i] = -100\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_encodings = mask_disorder(train_labels_encodings, training_disorder)\n",
    "casp12_labels_encodings = mask_disorder(casp12_labels_encodings, casp12_disorder)\n",
    "casp14_labels_encodings = mask_disorder(casp14_labels_encodings, casp14_disorder)\n",
    "ts115_labels_encodings = mask_disorder(ts115_labels_encodings, ts115_disorder)\n",
    "cb513_labels_encodings = mask_disorder(cb513_labels_encodings, cb513_disorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSPDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.encodings[idx]\n",
    "        labels = self.labels[idx]\n",
    "        return {'embed': torch.tensor(embedding), 'labels': torch.tensor(labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = SSPDataset(training_embeddings, train_labels_encodings[:10])\n",
    "casp12_dataset = SSPDataset(casp12_embeddings, casp12_labels_encodings[:10])\n",
    "casp14_dataset = SSPDataset(casp14_embeddings, casp14_labels_encodings[:10])\n",
    "ts115_dataset = SSPDataset(ts115_embeddings, ts115_labels_encodings[:10])\n",
    "cb513_dataset = SSPDataset(cb513_embeddings, cb513_labels_encodings[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba34483",
   "metadata": {},
   "source": [
    "### Function for computing metrics, Accuracy is used in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray):\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(id2tag[label_ids[i][j]])\n",
    "                    preds_list[i].append(id2tag[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(out_label_list, preds_list),\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4297c663",
   "metadata": {},
   "source": [
    "### Model initialization function for HuggingFace's trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(num_tokens, embed_dim):\n",
    "    hidden_dim = int(embed_dim / 2)\n",
    "    num_hidden_layers = 1 # Number of hidden layers in ConvBert.\n",
    "    nlayers = 1 # Number of ConvBert layers.\n",
    "    nhead = 4\n",
    "    dropout = 0.2\n",
    "    conv_kernel_size = 7\n",
    "    downstream_model = ankh.ConvBertForMultiClassClassification(num_tokens=num_tokens,\n",
    "                                                                input_dim=embed_dim, \n",
    "                                                                nhead=nhead, \n",
    "                                                                hidden_dim=hidden_dim, \n",
    "                                                                num_hidden_layers=num_hidden_layers, \n",
    "                                                                num_layers=nlayers, \n",
    "                                                                kernel_size=conv_kernel_size,\n",
    "                                                                dropout=dropout)\n",
    "    return downstream_model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6281f193",
   "metadata": {},
   "source": [
    "### Create and configure HuggingFace's TrainingArguments instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ankh_large'\n",
    "experiment = f'ssp3_{model_type}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results_{experiment}',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=1000,\n",
    "    learning_rate=1e-03,\n",
    "    weight_decay=0.0,\n",
    "    logging_dir=f'./logs_{experiment}',\n",
    "    logging_steps=200,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=False,\n",
    "    fp16_opt_level=\"02\",\n",
    "    run_name=experiment,\n",
    "    seed=seed,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f5c7d82",
   "metadata": {},
   "source": [
    "### Create HuggingFace Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed_dim = 1536 # Embedding dimension for ankh large.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=partial(model_init, num_tokens=len(unique_tags), embed_dim=model_embed_dim),\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=casp12_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1983f0e8",
   "metadata": {},
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521367e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c904201",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics_output = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954ed85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
