{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "sZqWrVCkXMow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GHTbbIrBo8J"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install git+https://github.com/agemagician/Ankh.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AgxX3SBtB21y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from datasets import load_dataset\n",
        "import ankh\n",
        "from transformers.models import convbert\n",
        "from keras import layers\n",
        "\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Ankh Base as a TensorFlow model."
      ],
      "metadata": {
        "id": "unvmHGvWTEoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = ankh.load_base_model(model_format='tf')"
      ],
      "metadata": {
        "id": "sb_atXXuitxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Fluorescence dataset from Proteinea's Hugging Face Datasets."
      ],
      "metadata": {
        "id": "YQnhiiIcTTGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('proteinea/fluorescence')\n",
        "\n",
        "train = dataset['train']\n",
        "validation = dataset['validation']\n",
        "test = dataset['test']"
      ],
      "metadata": {
        "id": "nDVwiB3cKFVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the tokenization function that will be used in `tf.data.Dataset` instance."
      ],
      "metadata": {
        "id": "kb4vBbrkTmD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x, y):\n",
        "  # `.decode(\"utf8\") is used because TensorFlow converts the strings to bytes,\n",
        "  # so we need to convert them back to string using `.decode(\"utf8\")` function.\n",
        "  x = tokenizer.encode(list(x.numpy().decode('utf8')), is_split_into_words=True, add_special_tokens=True)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "S3wPQUwUPVAl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WeLOueVHpWD6"
      },
      "outputs": [],
      "source": [
        "# Create our datasets using `.from_tensor_slices()` method.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train['primary'], train['log_fluorescence']))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test['primary'], test['log_fluorescence']))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((validation['primary'], validation['log_fluorescence']))\n",
        "\n",
        "# Map each residue in every protein sequence\n",
        "# to its corresponding id using `tokenize()` function.\n",
        "train_dataset = train_dataset.map(lambda x, y: tf.py_function(tokenize, inp=[x, y], Tout=(tf.int32, tf.float32)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Pad our sequences so that each sequence can have\n",
        "# the same length as the longest sequence in its current batch using `.padded_batch()`.\n",
        "train_dataset = train_dataset.padded_batch(16, padded_shapes=((None,), []))\n",
        "# Prefetch from our dataset.\n",
        "train_dataset = train_dataset.prefetch(1024)\n",
        "\n",
        "test_dataset = test_dataset.map(lambda x, y: tf.py_function(tokenize, inp=[x, y], Tout=(tf.int32, tf.float32)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.padded_batch(16, padded_shapes=((None,), []))\n",
        "test_dataset = test_dataset.prefetch(1024)\n",
        "\n",
        "valid_dataset = valid_dataset.map(lambda x, y: tf.py_function(tokenize, inp=[x, y], Tout=(tf.int32, tf.float32)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.padded_batch(16, padded_shapes=((None,), []))\n",
        "valid_dataset = valid_dataset.prefetch(1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the downstream model."
      ],
      "metadata": {
        "id": "dVZ-Il2zVvLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ConvBert configuration,\n",
        "# the same configuration that was used in the paper.\n",
        "convbert_config = convbert.ConvBertConfig(hidden_size=768,\n",
        "                                          num_hidden_layers=1,\n",
        "                                          num_attention_heads=8,\n",
        "                                          intermediate_size=768//2,\n",
        "                                          hidden_dropout_prob=0.1,\n",
        "                                          conv_kernel_size=7)\n",
        "\n",
        "# Freeze Ankh Base weights.\n",
        "model.trainable = False\n",
        "\n",
        "inputs = layers.Input((None,))\n",
        "\n",
        "# Pass the inputs layer to the model.\n",
        "x = model(inputs, training=False)\n",
        "\n",
        "# Pass the output layer (`last_hidden_state`) to the ConvBert Layer.\n",
        "x = convbert.TFConvBertLayer(convbert_config)(x.last_hidden_state, None, None, None)[0]\n",
        "# Apply Global Max Pooling over the timesteps.\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "output = layers.Dense(1, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=seed))(x)\n",
        "\n",
        "# Create our downstream model\n",
        "downstream_model = keras.Model(inputs, output)"
      ],
      "metadata": {
        "id": "3kZ_avkPNsPE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile our downstream model."
      ],
      "metadata": {
        "id": "RF7try1jVzhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "rAqWc9Tzpzf_"
      },
      "outputs": [],
      "source": [
        "downstream_model.compile(loss='mse', optimizer='adam', jit_compile=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train our downstream model."
      ],
      "metadata": {
        "id": "yOieqbbeWDYG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbleIamKp26f"
      },
      "outputs": [],
      "source": [
        "downstream_model.fit(train_dataset, validation_data=valid_dataset, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model."
      ],
      "metadata": {
        "id": "sahIxPcOW6Zi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24F5g11ruPyo"
      },
      "outputs": [],
      "source": [
        "downstream_model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ray4OPQVWHtz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}